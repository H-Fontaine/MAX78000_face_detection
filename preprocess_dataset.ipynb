{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_selection = 'test' # 'train' or 'val' or 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Preprocessing image 99 of 16097\n",
      "Preprocessing image 199 of 16097\n",
      "Preprocessing image 299 of 16097\n",
      "Preprocessing image 399 of 16097\n",
      "Preprocessing image 499 of 16097\n",
      "Preprocessing image 599 of 16097\n",
      "Preprocessing image 699 of 16097\n",
      "Preprocessing image 799 of 16097\n",
      "Preprocessing image 899 of 16097\n",
      "Preprocessing image 999 of 16097\n",
      "Preprocessing image 1099 of 16097\n",
      "Preprocessing image 1199 of 16097\n",
      "Preprocessing image 1299 of 16097\n",
      "Preprocessing image 1399 of 16097\n",
      "Preprocessing image 1499 of 16097\n",
      "Preprocessing image 1599 of 16097\n",
      "Preprocessing image 1699 of 16097\n",
      "Preprocessing image 1799 of 16097\n",
      "Preprocessing image 1899 of 16097\n",
      "Preprocessing image 1999 of 16097\n",
      "Preprocessing image 2099 of 16097\n",
      "Preprocessing image 2199 of 16097\n",
      "Preprocessing image 2299 of 16097\n",
      "Preprocessing image 2399 of 16097\n",
      "Preprocessing image 2499 of 16097\n",
      "Preprocessing image 2599 of 16097\n",
      "Preprocessing image 2699 of 16097\n",
      "Preprocessing image 2799 of 16097\n",
      "Preprocessing image 2899 of 16097\n",
      "Preprocessing image 2999 of 16097\n",
      "Preprocessing image 3099 of 16097\n",
      "Preprocessing image 3199 of 16097\n",
      "Preprocessing image 3299 of 16097\n",
      "Preprocessing image 3399 of 16097\n",
      "Preprocessing image 3499 of 16097\n",
      "Preprocessing image 3599 of 16097\n",
      "Preprocessing image 3699 of 16097\n",
      "Preprocessing image 3799 of 16097\n",
      "Preprocessing image 3899 of 16097\n",
      "Preprocessing image 3999 of 16097\n",
      "Preprocessing image 4099 of 16097\n",
      "Preprocessing image 4199 of 16097\n",
      "Preprocessing image 4299 of 16097\n",
      "Preprocessing image 4399 of 16097\n",
      "Preprocessing image 4499 of 16097\n",
      "Preprocessing image 4599 of 16097\n",
      "Preprocessing image 4699 of 16097\n",
      "Preprocessing image 4799 of 16097\n",
      "Preprocessing image 4899 of 16097\n",
      "Preprocessing image 4999 of 16097\n",
      "Preprocessing image 5099 of 16097\n",
      "Preprocessing image 5199 of 16097\n",
      "Preprocessing image 5299 of 16097\n",
      "Preprocessing image 5399 of 16097\n",
      "Preprocessing image 5499 of 16097\n",
      "Preprocessing image 5599 of 16097\n",
      "Preprocessing image 5699 of 16097\n",
      "Preprocessing image 5799 of 16097\n",
      "Preprocessing image 5899 of 16097\n",
      "Preprocessing image 5999 of 16097\n",
      "Preprocessing image 6099 of 16097\n",
      "Preprocessing image 6199 of 16097\n",
      "Preprocessing image 6299 of 16097\n",
      "Preprocessing image 6399 of 16097\n",
      "Preprocessing image 6499 of 16097\n",
      "Preprocessing image 6599 of 16097\n",
      "Preprocessing image 6699 of 16097\n",
      "Preprocessing image 6799 of 16097\n",
      "Preprocessing image 6899 of 16097\n",
      "Preprocessing image 6999 of 16097\n",
      "Preprocessing image 7099 of 16097\n",
      "Preprocessing image 7199 of 16097\n",
      "Preprocessing image 7299 of 16097\n",
      "Preprocessing image 7399 of 16097\n",
      "Preprocessing image 7499 of 16097\n",
      "Preprocessing image 7599 of 16097\n",
      "Preprocessing image 7699 of 16097\n",
      "Preprocessing image 7799 of 16097\n",
      "Preprocessing image 7899 of 16097\n",
      "Preprocessing image 7999 of 16097\n",
      "Preprocessing image 8099 of 16097\n",
      "Preprocessing image 8199 of 16097\n",
      "Preprocessing image 8299 of 16097\n",
      "Preprocessing image 8399 of 16097\n",
      "Preprocessing image 8499 of 16097\n",
      "Preprocessing image 8599 of 16097\n",
      "Preprocessing image 8699 of 16097\n",
      "Preprocessing image 8799 of 16097\n",
      "Preprocessing image 8899 of 16097\n",
      "Preprocessing image 8999 of 16097\n",
      "Preprocessing image 9099 of 16097\n",
      "Preprocessing image 9199 of 16097\n",
      "Preprocessing image 9299 of 16097\n",
      "Preprocessing image 9399 of 16097\n",
      "Preprocessing image 9499 of 16097\n",
      "Preprocessing image 9599 of 16097\n",
      "Preprocessing image 9699 of 16097\n",
      "Preprocessing image 9799 of 16097\n",
      "Preprocessing image 9899 of 16097\n",
      "Preprocessing image 9999 of 16097\n",
      "Preprocessing image 10099 of 16097\n",
      "Preprocessing image 10199 of 16097\n",
      "Preprocessing image 10299 of 16097\n",
      "Preprocessing image 10399 of 16097\n",
      "Preprocessing image 10499 of 16097\n",
      "Preprocessing image 10599 of 16097\n",
      "Preprocessing image 10699 of 16097\n",
      "Preprocessing image 10799 of 16097\n",
      "Preprocessing image 10899 of 16097\n",
      "Preprocessing image 10999 of 16097\n",
      "Preprocessing image 11099 of 16097\n",
      "Preprocessing image 11199 of 16097\n",
      "Preprocessing image 11299 of 16097\n",
      "Preprocessing image 11399 of 16097\n",
      "Preprocessing image 11499 of 16097\n",
      "Preprocessing image 11599 of 16097\n",
      "Preprocessing image 11699 of 16097\n",
      "Preprocessing image 11799 of 16097\n",
      "Preprocessing image 11899 of 16097\n",
      "Preprocessing image 11999 of 16097\n",
      "Preprocessing image 12099 of 16097\n",
      "Preprocessing image 12199 of 16097\n",
      "Preprocessing image 12299 of 16097\n",
      "Preprocessing image 12399 of 16097\n",
      "Preprocessing image 12499 of 16097\n",
      "Preprocessing image 12599 of 16097\n",
      "Preprocessing image 12699 of 16097\n",
      "Preprocessing image 12799 of 16097\n",
      "Preprocessing image 12899 of 16097\n",
      "Preprocessing image 12999 of 16097\n",
      "Preprocessing image 13099 of 16097\n",
      "Preprocessing image 13199 of 16097\n",
      "Preprocessing image 13299 of 16097\n",
      "Preprocessing image 13399 of 16097\n",
      "Preprocessing image 13499 of 16097\n",
      "Preprocessing image 13599 of 16097\n",
      "Preprocessing image 13699 of 16097\n",
      "Preprocessing image 13799 of 16097\n",
      "Preprocessing image 13899 of 16097\n",
      "Preprocessing image 13999 of 16097\n",
      "Preprocessing image 14099 of 16097\n",
      "Preprocessing image 14199 of 16097\n",
      "Preprocessing image 14299 of 16097\n",
      "Preprocessing image 14399 of 16097\n",
      "Preprocessing image 14499 of 16097\n",
      "Preprocessing image 14599 of 16097\n",
      "Preprocessing image 14699 of 16097\n",
      "Preprocessing image 14799 of 16097\n",
      "Preprocessing image 14899 of 16097\n",
      "Preprocessing image 14999 of 16097\n",
      "Preprocessing image 15099 of 16097\n",
      "Preprocessing image 15199 of 16097\n",
      "Preprocessing image 15299 of 16097\n",
      "Preprocessing image 15399 of 16097\n",
      "Preprocessing image 15499 of 16097\n",
      "Preprocessing image 15599 of 16097\n",
      "Preprocessing image 15699 of 16097\n",
      "Preprocessing image 15799 of 16097\n",
      "Preprocessing image 15899 of 16097\n",
      "Preprocessing image 15999 of 16097\n",
      "Number of faces: 0\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import WIDERFace\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as trF\n",
    "import torch.nn.functional as nnF\n",
    "import os\n",
    "import random as rd\n",
    "\n",
    "# Set paths and other constants\n",
    "dataset_path = '/itet-stor/hfontaine/net_scratch/datasets'\n",
    "\n",
    "if not os.path.exists(os.path.join(dataset_path, 'PreprocessedWider')):\n",
    "    os.makedirs(os.path.join(dataset_path, 'PreprocessedWider'))\n",
    "if not os.path.exists(os.path.join(dataset_path, 'PreprocessedWider', split_selection)):\n",
    "    os.makedirs(os.path.join(dataset_path, 'PreprocessedWider', split_selection))\n",
    "# else : #stop execution if the folder already exists\n",
    "#     print('Folder already exists')\n",
    "#     exit()\n",
    "    \n",
    "\n",
    "IMAGE_WIDTH_CROP = 500\n",
    "IMAGE_HEIGHT_CROP = 500\n",
    "IMAGE_WIDHT_RESIZE = 64\n",
    "IMAGE_HEIGHT_RESIZE = 64\n",
    "NORMALIZED_MEAN = [0.485, 0.456, 0.406]\n",
    "NORMALIZED_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "dataset = WIDERFace(root=dataset_path, split=split_selection, download=True)\n",
    "face_count = 0\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    image, target = dataset[i]\n",
    "    image = transforms.ToTensor()(image)\n",
    "    im_height, im_width = image.size()[1:]\n",
    "\n",
    "    # Pad image if smaller than required size\n",
    "    padl = padr = padt = padb = 0\n",
    "    if im_width < IMAGE_WIDTH_CROP:\n",
    "        padl = (IMAGE_WIDTH_CROP - im_width) // 2\n",
    "        padr = IMAGE_WIDTH_CROP - im_width - padl\n",
    "        image = nnF.pad(image, (padl, padr), mode='constant', value=0)\n",
    "        im_height, im_width = image.size()[1:]\n",
    "    if im_height < IMAGE_HEIGHT_CROP:\n",
    "        padt = (IMAGE_HEIGHT_CROP - im_height) // 2\n",
    "        padb = IMAGE_HEIGHT_CROP - im_height - padt\n",
    "        image = nnF.pad(image, (0, 0, padt, padb), mode='constant', value=0)\n",
    "        im_height, im_width = image.size()[1:]\n",
    "    \n",
    "    # Crop image at random location\n",
    "    crop_x = rd.randint(0, im_width - IMAGE_WIDTH_CROP)\n",
    "    crop_y = rd.randint(0, im_height - IMAGE_HEIGHT_CROP)\n",
    "    image = trF.crop(image, crop_y, crop_x, IMAGE_HEIGHT_CROP, IMAGE_WIDTH_CROP)\n",
    "    image = trF.resize(image, (IMAGE_HEIGHT_RESIZE, IMAGE_WIDHT_RESIZE))\n",
    "    image = transforms.ToPILImage(mode='RGB')(image)\n",
    "\n",
    "    # Transform bounding boxes\n",
    "    is_face = 'X'\n",
    "    if split_selection != 'test':\n",
    "        is_face = 0\n",
    "        for j in range(len(target['bbox'])):\n",
    "            x, y, w, h = target['bbox'][j]\n",
    "            x = int(x) + padl\n",
    "            y = int(y) + padt\n",
    "            w = int(w)\n",
    "            h = int(h)\n",
    "            if x >= crop_x + IMAGE_WIDTH_CROP or y >= crop_y + IMAGE_HEIGHT_CROP:\n",
    "                continue\n",
    "            if x + w <= crop_x or y + h <= crop_y:\n",
    "                continue\n",
    "            x1 = max(crop_x, x)\n",
    "            w_new = w - (x1 - x)\n",
    "            y1 = max(crop_y, y)\n",
    "            h_new = h - (y1 - y)\n",
    "            if x1 + w_new > crop_x + IMAGE_WIDTH_CROP:\n",
    "                w_new = crop_x + IMAGE_WIDTH_CROP - x1\n",
    "            if y1 + h_new > crop_y + IMAGE_HEIGHT_CROP:\n",
    "                h_new = crop_y + IMAGE_HEIGHT_CROP - y1\n",
    "            area_new = w_new * h_new\n",
    "            if area_new * (IMAGE_HEIGHT_RESIZE / IMAGE_HEIGHT_CROP) * (IMAGE_WIDHT_RESIZE / IMAGE_WIDTH_CROP) >= 9 and area_new >= 0.7 * w * h:\n",
    "                is_face = 1\n",
    "                face_count += 1\n",
    "                break\n",
    "    \n",
    "    # Save the jpeg image and the is_face label in the name of the file\n",
    "    image.save(os.path.join(dataset_path, 'PreprocessedWider', split_selection, 'image_' + str(i) + '_' + str(is_face) + '.jpeg'))\n",
    "    if (i+1) % 100 == 0:\n",
    "        print('Preprocessing image ' + str(i) + ' of ' + str(len(dataset)))\n",
    "\n",
    "print('Number of faces: ' + str(face_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'X'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 64\u001b[0m\n\u001b[1;32m     62\u001b[0m fig\u001b[38;5;241m.\u001b[39msubplots_adjust(hspace\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, wspace\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m9\u001b[39m):\n\u001b[0;32m---> 64\u001b[0m     image, label \u001b[38;5;241m=\u001b[39m dataset[rd\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(dataset) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m     65\u001b[0m     image \u001b[38;5;241m=\u001b[39m unnormalize_image(image, torch\u001b[38;5;241m.\u001b[39mtensor(NORMALIZED_MEAN), torch\u001b[38;5;241m.\u001b[39mtensor(NORMALIZED_STD))\n\u001b[1;32m     66\u001b[0m     ax \u001b[38;5;241m=\u001b[39m fig\u001b[38;5;241m.\u001b[39madd_subplot(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m, i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 37\u001b[0m, in \u001b[0;36mPreprocessedWiderDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     35\u001b[0m image \u001b[38;5;241m=\u001b[39m trF\u001b[38;5;241m.\u001b[39mnormalize(image, NORMALIZED_MEAN, NORMALIZED_STD)  \u001b[38;5;66;03m# Normalize image\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Extract the label from the filename (before the '.jpeg' extension)\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(image_name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m])  \u001b[38;5;66;03m# 'image_00_1.jpeg' -> label = 1\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label :\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image, torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;66;03m#[1, 0] for face\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'X'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1850x1050 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import torchvision.transforms.functional as trF\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rd\n",
    "\n",
    "\n",
    "IMAGE_WIDTH = 64\n",
    "IMAGE_HEIGHT = 64\n",
    "NORMALIZED_MEAN = [0.485, 0.456, 0.406]\n",
    "NORMALIZED_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Set paths and other constants\n",
    "dataset_path = '/itet-stor/hfontaine/net_scratch/datasets'\n",
    "preprocessed_path = os.path.join(dataset_path, 'PreprocessedWider', split_selection)\n",
    "\n",
    "# Custom dataset class to read preprocessed images\n",
    "class PreprocessedWiderDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = [f for f in os.listdir(root_dir) if f.endswith('.jpeg')]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the image and label\n",
    "        image_name = self.image_files[idx]\n",
    "        image_path = os.path.join(self.root_dir, image_name)\n",
    "        image = Image.open(image_path)  # Open image\n",
    "        image = trF.to_tensor(image)  # Convert image to tensor\n",
    "        image = trF.normalize(image, NORMALIZED_MEAN, NORMALIZED_STD)  # Normalize image\n",
    "        # Extract the label from the filename (before the '.jpeg' extension)\n",
    "        label = int(image_name.split('_')[-1][0])  # 'image_00_1.jpeg' -> label = 1\n",
    "        if label :\n",
    "            return image, torch.tensor([1, 0], dtype=torch.float32) #[1, 0] for face\n",
    "        else:\n",
    "            return image, torch.tensor([0, 1], dtype=torch.float32) #[0, 1] for no face\n",
    "\n",
    "# Instantiate the dataset and dataloader\n",
    "dataset = PreprocessedWiderDataset(root_dir=preprocessed_path)\n",
    "\n",
    "def unnormalize_image(image, mean, std):\n",
    "    \"\"\"\n",
    "    Reverse the normalization process for a single image.\n",
    "    Args:\n",
    "        image (Tensor): Normalized image tensor of shape (C, H, W)\n",
    "        mean (Tensor): Mean values used during normalization\n",
    "        std (Tensor): Std values used during normalization\n",
    "    Returns:\n",
    "        Tensor: Unnormalized image tensor of shape (C, H, W)\n",
    "    \"\"\"\n",
    "    mean = mean[:, None, None]  # Adjust shape to match (C, H, W)\n",
    "    std = std[:, None, None]\n",
    "    return image * std + mean\n",
    "\n",
    "#show some images in figure with ther bbox\n",
    "fig = plt.figure(figsize=(18.5, 10.5))\n",
    "fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
    "for i in range(9):\n",
    "    image, label = dataset[rd.randint(0, len(dataset) - 1)]\n",
    "    image = unnormalize_image(image, torch.tensor(NORMALIZED_MEAN), torch.tensor(NORMALIZED_STD))\n",
    "    ax = fig.add_subplot(3, 3, i + 1)\n",
    "    ax.imshow(image.permute(1, 2, 0))\n",
    "    ax.title.set_text(\"Face\" if label[0] == 1 else \"No face\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cluster",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
