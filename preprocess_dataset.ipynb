{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_selection = 'test' # 'train' or 'val' or 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Preprocessing image 99 of 16097\n",
      "Preprocessing image 199 of 16097\n",
      "Preprocessing image 299 of 16097\n",
      "Preprocessing image 399 of 16097\n",
      "Preprocessing image 499 of 16097\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataset)):\n\u001b[1;32m     31\u001b[0m     image, target \u001b[38;5;241m=\u001b[39m dataset[i]\n\u001b[0;32m---> 32\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mToTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     im_height, im_width \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# Pad image if smaller than required size\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mlmc/lib/python3.11/site-packages/torchvision/transforms/transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mlmc/lib/python3.11/site-packages/torchvision/transforms/functional.py:168\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# handle PIL Image\u001b[39;00m\n\u001b[1;32m    167\u001b[0m mode_to_nptype \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mint32, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI;16\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mbyteorder \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlittle\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI;16B\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mint16, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mfloat32}\n\u001b[0;32m--> 168\u001b[0m img \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39marray(pic, mode_to_nptype\u001b[38;5;241m.\u001b[39mget(pic\u001b[38;5;241m.\u001b[39mmode, np\u001b[38;5;241m.\u001b[39muint8), copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    171\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;241m*\u001b[39m img\n",
      "File \u001b[0;32m~/anaconda3/envs/mlmc/lib/python3.11/site-packages/PIL/Image.py:756\u001b[0m, in \u001b[0;36mImage.__array_interface__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    754\u001b[0m     new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtobytes(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 756\u001b[0m     new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtobytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    757\u001b[0m new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m], new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtypestr\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _conv_type_shape(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    758\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new\n",
      "File \u001b[0;32m~/anaconda3/envs/mlmc/lib/python3.11/site-packages/PIL/Image.py:805\u001b[0m, in \u001b[0;36mImage.tobytes\u001b[0;34m(self, encoder_name, *args)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoder_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m encoder_args \u001b[38;5;241m==\u001b[39m ():\n\u001b[1;32m    803\u001b[0m     encoder_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\n\u001b[0;32m--> 805\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwidth \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheight \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    808\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/mlmc/lib/python3.11/site-packages/PIL/ImageFile.py:300\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[1;32m    299\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 300\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import WIDERFace\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as trF\n",
    "import torch.nn.functional as nnF\n",
    "import os\n",
    "import random as rd\n",
    "\n",
    "# Set paths and other constants\n",
    "dataset_path = '/itet-stor/hfontaine/net_scratch/datasets'\n",
    "\n",
    "if not os.path.exists(os.path.join(dataset_path, 'PreprocessedWider')):\n",
    "    os.makedirs(os.path.join(dataset_path, 'PreprocessedWider'))\n",
    "if not os.path.exists(os.path.join(dataset_path, 'PreprocessedWider', split_selection)):\n",
    "    os.makedirs(os.path.join(dataset_path, 'PreprocessedWider', split_selection))\n",
    "\n",
    "\n",
    "IMAGE_WIDTH_CROP = 500\n",
    "IMAGE_HEIGHT_CROP = 500\n",
    "IMAGE_WIDHT_RESIZE = 64\n",
    "IMAGE_HEIGHT_RESIZE = 64\n",
    "NORMALIZED_MEAN = [0.485, 0.456, 0.406]\n",
    "NORMALIZED_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "dataset = WIDERFace(root=dataset_path, split=split_selection, download=True)\n",
    "face_count = 0\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    image, target = dataset[i]\n",
    "    image = transforms.ToTensor()(image)\n",
    "    im_height, im_width = image.size()[1:]\n",
    "\n",
    "    # Pad image if smaller than required size\n",
    "    padl = padr = padt = padb = 0\n",
    "    if im_width < IMAGE_WIDTH_CROP:\n",
    "        padl = (IMAGE_WIDTH_CROP - im_width) // 2\n",
    "        padr = IMAGE_WIDTH_CROP - im_width - padl\n",
    "        image = nnF.pad(image, (padl, padr), mode='constant', value=0)\n",
    "        im_height, im_width = image.size()[1:]\n",
    "    if im_height < IMAGE_HEIGHT_CROP:\n",
    "        padt = (IMAGE_HEIGHT_CROP - im_height) // 2\n",
    "        padb = IMAGE_HEIGHT_CROP - im_height - padt\n",
    "        image = nnF.pad(image, (0, 0, padt, padb), mode='constant', value=0)\n",
    "        im_height, im_width = image.size()[1:]\n",
    "    \n",
    "    # Crop image at random location\n",
    "    crop_x = rd.randint(0, im_width - IMAGE_WIDTH_CROP)\n",
    "    crop_y = rd.randint(0, im_height - IMAGE_HEIGHT_CROP)\n",
    "    image = trF.crop(image, crop_y, crop_x, IMAGE_HEIGHT_CROP, IMAGE_WIDTH_CROP)\n",
    "    image = trF.resize(image, (IMAGE_HEIGHT_RESIZE, IMAGE_WIDHT_RESIZE))\n",
    "    image = transforms.ToPILImage(mode='RGB')(image)\n",
    "\n",
    "    # Transform bounding boxes\n",
    "    is_face = 'X'\n",
    "    if split_selection != 'test':\n",
    "        is_face = 0\n",
    "        for j in range(len(target['bbox'])):\n",
    "            x, y, w, h = target['bbox'][j]\n",
    "            x = int(x) + padl\n",
    "            y = int(y) + padt\n",
    "            w = int(w)\n",
    "            h = int(h)\n",
    "            if x >= crop_x + IMAGE_WIDTH_CROP or y >= crop_y + IMAGE_HEIGHT_CROP:\n",
    "                continue\n",
    "            if x + w <= crop_x or y + h <= crop_y:\n",
    "                continue\n",
    "            x1 = max(crop_x, x)\n",
    "            w_new = w - (x1 - x)\n",
    "            y1 = max(crop_y, y)\n",
    "            h_new = h - (y1 - y)\n",
    "            if x1 + w_new > crop_x + IMAGE_WIDTH_CROP:\n",
    "                w_new = crop_x + IMAGE_WIDTH_CROP - x1\n",
    "            if y1 + h_new > crop_y + IMAGE_HEIGHT_CROP:\n",
    "                h_new = crop_y + IMAGE_HEIGHT_CROP - y1\n",
    "            area_new = w_new * h_new\n",
    "            if area_new * (IMAGE_HEIGHT_RESIZE / IMAGE_HEIGHT_CROP) * (IMAGE_WIDHT_RESIZE / IMAGE_WIDTH_CROP) >= 9 and area_new >= 0.7 * w * h:\n",
    "                is_face = 1\n",
    "                face_count += 1\n",
    "                break\n",
    "    \n",
    "    # Save the jpeg image and the is_face label in the name of the file\n",
    "    image.save(os.path.join(dataset_path, 'PreprocessedWider', split_selection, 'image_' + str(i) + '_' + str(is_face) + '.jpeg'))\n",
    "    if (i+1) % 100 == 0:\n",
    "        print('Preprocessing image ' + str(i) + ' of ' + str(len(dataset)))\n",
    "\n",
    "print('Number of faces: ' + str(face_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'X'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 64\u001b[0m\n\u001b[1;32m     62\u001b[0m fig\u001b[38;5;241m.\u001b[39msubplots_adjust(hspace\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, wspace\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m9\u001b[39m):\n\u001b[0;32m---> 64\u001b[0m     image, label \u001b[38;5;241m=\u001b[39m dataset[rd\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(dataset) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m     65\u001b[0m     image \u001b[38;5;241m=\u001b[39m unnormalize_image(image, torch\u001b[38;5;241m.\u001b[39mtensor(NORMALIZED_MEAN), torch\u001b[38;5;241m.\u001b[39mtensor(NORMALIZED_STD))\n\u001b[1;32m     66\u001b[0m     ax \u001b[38;5;241m=\u001b[39m fig\u001b[38;5;241m.\u001b[39madd_subplot(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m, i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 37\u001b[0m, in \u001b[0;36mPreprocessedWiderDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     35\u001b[0m image \u001b[38;5;241m=\u001b[39m trF\u001b[38;5;241m.\u001b[39mnormalize(image, NORMALIZED_MEAN, NORMALIZED_STD)  \u001b[38;5;66;03m# Normalize image\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Extract the label from the filename (before the '.jpeg' extension)\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(image_name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m])  \u001b[38;5;66;03m# 'image_00_1.jpeg' -> label = 1\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label :\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image, torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;66;03m#[1, 0] for face\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'X'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1850x1050 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import torchvision.transforms.functional as trF\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rd\n",
    "\n",
    "\n",
    "IMAGE_WIDTH = 64\n",
    "IMAGE_HEIGHT = 64\n",
    "NORMALIZED_MEAN = [0.485, 0.456, 0.406]\n",
    "NORMALIZED_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Set paths and other constants\n",
    "dataset_path = '/itet-stor/hfontaine/net_scratch/datasets'\n",
    "preprocessed_path = os.path.join(dataset_path, 'PreprocessedWider', split_selection)\n",
    "\n",
    "# Custom dataset class to read preprocessed images\n",
    "class PreprocessedWiderDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = [f for f in os.listdir(root_dir) if f.endswith('.jpeg')]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the image and label\n",
    "        image_name = self.image_files[idx]\n",
    "        image_path = os.path.join(self.root_dir, image_name)\n",
    "        image = Image.open(image_path)  # Open image\n",
    "        image = trF.to_tensor(image)  # Convert image to tensor\n",
    "        image = trF.normalize(image, NORMALIZED_MEAN, NORMALIZED_STD)  # Normalize image\n",
    "        # Extract the label from the filename (before the '.jpeg' extension)\n",
    "        label = int(image_name.split('_')[-1][0])  # 'image_00_1.jpeg' -> label = 1\n",
    "        if label :\n",
    "            return image, torch.tensor([1, 0], dtype=torch.float32) #[1, 0] for face\n",
    "        else:\n",
    "            return image, torch.tensor([0, 1], dtype=torch.float32) #[0, 1] for no face\n",
    "\n",
    "# Instantiate the dataset and dataloader\n",
    "dataset = PreprocessedWiderDataset(root_dir=preprocessed_path)\n",
    "\n",
    "def unnormalize_image(image, mean, std):\n",
    "    \"\"\"\n",
    "    Reverse the normalization process for a single image.\n",
    "    Args:\n",
    "        image (Tensor): Normalized image tensor of shape (C, H, W)\n",
    "        mean (Tensor): Mean values used during normalization\n",
    "        std (Tensor): Std values used during normalization\n",
    "    Returns:\n",
    "        Tensor: Unnormalized image tensor of shape (C, H, W)\n",
    "    \"\"\"\n",
    "    mean = mean[:, None, None]  # Adjust shape to match (C, H, W)\n",
    "    std = std[:, None, None]\n",
    "    return image * std + mean\n",
    "\n",
    "#show some images in figure with ther bbox\n",
    "fig = plt.figure(figsize=(18.5, 10.5))\n",
    "fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
    "for i in range(9):\n",
    "    image, label = dataset[rd.randint(0, len(dataset) - 1)]\n",
    "    image = unnormalize_image(image, torch.tensor(NORMALIZED_MEAN), torch.tensor(NORMALIZED_STD))\n",
    "    ax = fig.add_subplot(3, 3, i + 1)\n",
    "    ax.imshow(image.permute(1, 2, 0))\n",
    "    ax.title.set_text(\"Face\" if label[0] == 1 else \"No face\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlmc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
